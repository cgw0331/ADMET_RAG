#!/usr/bin/env python3
"""
YOLOë¡œ ì¶”ì¶œí•œ í‘œ/ê·¸ë¦¼ ì´ë¯¸ì§€ë¥¼ GPT-4o Visionìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í™”í•©ë¬¼ ì •ë³´ ì¶”ì¶œ
- supp_extracted/PMC###/pdf_graph/###/figures/, tables/ ì´ë¯¸ì§€ ë¶„ì„
- í™”í•©ë¬¼ ID (CBK), SMILES, Well Position ë“± ì¶”ì¶œ
- JSON Lines í˜•ì‹ìœ¼ë¡œ í†µí•© ì €ì¥
"""

import json
import logging
import base64
from pathlib import Path
from typing import Dict, List, Any, Optional
from PIL import Image
from openai import OpenAI
from dotenv import load_dotenv
import os
import io

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class YOLOImageAnalyzer:
    """YOLOë¡œ ì¶”ì¶œí•œ ì´ë¯¸ì§€ë¥¼ GPT-4o Visionìœ¼ë¡œ ë¶„ì„"""
    
    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        self.client = OpenAI(api_key=api_key)
    
    def find_extracted_images(self, pdf_graph_dir: Path) -> List[Dict[str, Any]]:
        """YOLOë¡œ ì¶”ì¶œëœ figures/ì™€ tables/ í´ë”ì˜ ì´ë¯¸ì§€ ì°¾ê¸°"""
        images = []
        
        # figures í´ë” ì°¾ê¸°
        figures_dir = pdf_graph_dir / "figures"
        if figures_dir.exists() and figures_dir.is_dir():
            for png_file in sorted(figures_dir.glob("*.png")):
                try:
                    img = Image.open(png_file)
                    images.append({
                        'image': img,
                        'filename': png_file.name,
                        'class': 'figure',
                        'file_path': str(png_file)
                    })
                    logger.debug(f"Figure ë°œê²¬: {png_file.name}")
                except Exception as e:
                    logger.warning(f"Figure ë¡œë“œ ì‹¤íŒ¨ {png_file}: {e}")
        
        # tables í´ë” ì°¾ê¸°
        tables_dir = pdf_graph_dir / "tables"
        if tables_dir.exists() and tables_dir.is_dir():
            for png_file in sorted(tables_dir.glob("*.png")):
                try:
                    img = Image.open(png_file)
                    images.append({
                        'image': img,
                        'filename': png_file.name,
                        'class': 'table',
                        'file_path': str(png_file)
                    })
                    logger.debug(f"Table ë°œê²¬: {png_file.name}")
                except Exception as e:
                    logger.warning(f"Table ë¡œë“œ ì‹¤íŒ¨ {png_file}: {e}")
        
        logger.info(f"ì´ {len(images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬ (Figures: {sum(1 for x in images if x['class']=='figure')}, Tables: {sum(1 for x in images if x['class']=='table')})")
        return images
    
    def analyze_image(self, image_data: Dict[str, Any], 
                     previous_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ GPT-4o Visionìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í™”í•©ë¬¼ ì •ë³´ ì¶”ì¶œ
        
        Args:
            image_data: ì´ë¯¸ì§€ ë°ì´í„° (image, class, filename)
            previous_context: ì´ì „ ë‹¨ê³„ì—ì„œ ëˆ„ì ëœ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
        """
        image = image_data['image']
        obj_class = image_data['class']
        filename = image_data['filename']
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        # ì´ì „ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context_section = ""
        if previous_context and previous_context.get("compounds"):
            compounds = previous_context["compounds"]
            compound_list = list(compounds.keys())[:50]  # ìµœëŒ€ 50ê°œ
            context_section = f"""

**ì´ì „ ë‹¨ê³„ì—ì„œ ë°œê²¬ëœ í™”í•©ë¬¼ë“¤ (ì°¸ê³ ìš©):**
{', '.join(compound_list) if compound_list else "None"}
- ì´ í™”í•©ë¬¼ë“¤ì´ ì´ë¯¸ì§€ì— ë‚˜íƒ€ë‚˜ë©´ ê¸°ì¡´ ì •ë³´ì™€ ë§¤ì¹­í•˜ì„¸ìš”
- ìƒˆë¡œìš´ í™”í•©ë¬¼ë„ ì¶”ê°€ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
"""
        
        prompt = f"""ì´ ì´ë¯¸ì§€ëŠ” ê³¼í•™ ë…¼ë¬¸ ë³´ì¶©ìë£Œì˜ {obj_class}ì…ë‹ˆë‹¤ (íŒŒì¼ëª…: {filename}).
{context_section}
**ì‘ì—…:**
ì´ {obj_class}ì—ì„œ **ëª¨ë“  í™”í•©ë¬¼ ì •ë³´**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶”ì¶œ í˜•ì‹ (JSON Lines):**
ê° ë ˆì½”ë“œë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥:
{{"compound_name": "í™”í•©ë¬¼ID/ì´ë¦„", "indicator_name": "ì†ì„±ëª…", "value": "ê°’", "unit": "", "source": "{filename}"}}

**ì¤‘ìš” ì§€ì‹œì‚¬í•­:**
1. {obj_class}ì˜ **ëª¨ë“  í–‰(row)**ì„ ë¹ ì§ì—†ì´ ë¶„ì„
2. í™”í•©ë¬¼ ID (ì˜ˆ: CBK037537, CBK093726, CBK074456 ë“±) ì¶”ì¶œ
3. Well ìœ„ì¹˜ (ì˜ˆ: "2 B01", "2 C01", "1 A01" ë“±) ì¶”ì¶œ
4. SMILES êµ¬ì¡°ì‹ ì¶”ì¶œ
5. ADMET í•„í„°ë§í•˜ì§€ ë§ê³  **ëª¨ë“  ì†ì„±** í¬í•¨
6. í‘œ ìº¡ì…˜/ì œëª©ë„ sourceì— í¬í•¨ ê°€ëŠ¥í•˜ë©´ í¬í•¨

**ì˜ˆì‹œ:**
í‘œì— "2 B01 CBK037537 NC1=CC(=CC=C1)C1=NC=CN=C1NCC1=CC=CS1" ì´ ìˆìœ¼ë©´:
{{"compound_name": "CBK037537", "indicator_name": "Well Position", "value": "2 B01", "unit": "", "source": "{filename}"}}
{{"compound_name": "CBK037537", "indicator_name": "SMILES", "value": "NC1=CC(=CC=C1)C1=NC=CN=C1NCC1=CC=CS1", "unit": "", "source": "{filename}"}}

JSON Lines í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥ (ê° ë ˆì½”ë“œê°€ í•œ ì¤„).
í™”í•©ë¬¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”."""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                temperature=0,
                max_tokens=4000
            )
            
            generated_text = response.choices[0].message.content
            records = self._parse_jsonl(generated_text, filename)
            
            if not records:
                logger.debug(f"{filename}: í™”í•©ë¬¼ ì •ë³´ ì—†ìŒ")
            
            return records
                    
        except Exception as e:
            logger.error(f"{filename} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_image_with_history(self, image_data: Dict[str, Any], 
                                   messages: List[Dict[str, Any]],
                                   image_index: int, total_images: int) -> List[Dict[str, Any]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì´ë¯¸ì§€ ë¶„ì„ (ChatGPTì²˜ëŸ¼ ì»¨í…ìŠ¤íŠ¸ ì´ì–´ì§)"""
        image = image_data['image']
        obj_class = image_data['class']
        filename = image_data['filename']
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ (ì´ë¯¸ì§€ í¬í•¨)
        user_prompt = f"""ì´ë¯¸ì§€ {image_index}/{total_images}: ê³¼í•™ ë…¼ë¬¸ ë³´ì¶©ìë£Œì˜ {obj_class} (íŒŒì¼ëª…: {filename})

**ì‘ì—…:**
ì´ {obj_class}ì—ì„œ **ëª¨ë“  í™”í•©ë¬¼ ì •ë³´**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¤‘ìš”:**
- ì´ì „ ì´ë¯¸ì§€ë“¤ì—ì„œ ë°œê²¬ëœ í™”í•©ë¬¼ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°, ë™ì¼í•œ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ìƒˆë¡œìš´ í™”í•©ë¬¼ë„ ì¶”ê°€ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
- {obj_class}ì˜ **ëª¨ë“  í–‰(row)**ì„ ë¹ ì§ì—†ì´ ë¶„ì„

**ì¶”ì¶œ í˜•ì‹ (JSON Lines):**
ê° ë ˆì½”ë“œë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥:
{{"compound_name": "í™”í•©ë¬¼ID/ì´ë¦„", "indicator_name": "ì†ì„±ëª…", "value": "ê°’", "unit": "", "source": "{filename}"}}

**ì˜ˆì‹œ:**
í‘œì— "2 B01 CBK037537 NC1=CC(=CC=C1)C1=NC=CN=C1NCC1=CC=CS1" ì´ ìˆìœ¼ë©´:
{{"compound_name": "CBK037537", "indicator_name": "Well Position", "value": "2 B01", "unit": "", "source": "{filename}"}}
{{"compound_name": "CBK037537", "indicator_name": "SMILES", "value": "NC1=CC(=CC=C1)C1=NC=CN=C1NCC1=CC=CS1", "unit": "", "source": "{filename}"}}

JSON Lines í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥ (ê° ë ˆì½”ë“œê°€ í•œ ì¤„).
í™”í•©ë¬¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”."""
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({
            "role": "user",
            "content": [
                {"type": "text", "text": user_prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_base64}"
                    }
                }
            ]
        })
        
        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,  # ì´ì „ ëŒ€í™” í¬í•¨
                temperature=0,
                max_tokens=4000
            )
            
            generated_text = response.choices[0].message.content
            records = self._parse_jsonl(generated_text, filename)
            
            # Assistant ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë‹¤ìŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹œ ì°¸ì¡°)
            messages.append({
                "role": "assistant",
                "content": generated_text
            })
            
            if not records:
                logger.debug(f"{filename}: í™”í•©ë¬¼ ì •ë³´ ì—†ìŒ")
            
            return records
                    
        except Exception as e:
            logger.error(f"{filename} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_jsonl(self, text: str, source: str) -> List[Dict[str, Any]]:
        """JSON Lines íŒŒì‹±"""
        records = []
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        for line in lines:
            if line.startswith('```'):
                continue
            if '```json' in line:
                line = line.replace('```json', '').strip()
            if line.endswith('```'):
                line = line[:-3].strip()
            
            try:
                record = json.loads(line)
                # source í•„ë“œ ë³´ì •
                if 'source' not in record or not record['source']:
                    record['source'] = source
                elif source not in record['source']:
                    record['source'] = f"{source}/{record['source']}"
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if "compound_name" in record and "indicator_name" in record:
                    records.append(record)
            except json.JSONDecodeError:
                continue
        
        return records
    
    def analyze_pdf_graph(self, pdf_graph_dir: Path, 
                         previous_context: Optional[Dict[str, Any]] = None,
                         use_conversation_history: bool = True) -> Dict[str, Any]:
        """PDF ê·¸ë˜í”„ í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ ë¶„ì„ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€)
        
        Args:
            pdf_graph_dir: PDF ê·¸ë˜í”„ ë””ë ‰í† ë¦¬
            previous_context: ì´ì „ ë‹¨ê³„ì—ì„œ ëˆ„ì ëœ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            use_conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€ ì—¬ë¶€ (Trueë©´ ChatGPTì²˜ëŸ¼ ì»¨í…ìŠ¤íŠ¸ ì´ì–´ì§)
        """
        logger.info(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {pdf_graph_dir}")
        
        # 1. ì´ë¯¸ì§€ ì°¾ê¸°
        images = self.find_extracted_images(pdf_graph_dir)
        
        if not images:
            logger.warning("ë¶„ì„í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "compounds": [],
                "summary": {
                    "total_compounds": 0,
                    "total_attributes": 0,
                    "total_records": 0,
                    "images_processed": 0
                },
                "raw_jsonl": []
            }
        
        # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì´ì „ ì»¨í…ìŠ¤íŠ¸)
        messages = []
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì´ì „ ë‹¨ê³„ì—ì„œ ë°œê²¬ëœ í™”í•©ë¬¼ ì •ë³´ í¬í•¨
        system_content = """You are an expert data extractor for biomedical ADMET compounds.
Extract ALL compounds and their attributes from each image.
Output in JSON Lines format: {"compound_name": "...", "indicator_name": "...", "value": "...", "unit": "", "source": "..."}"""
        
        if previous_context and previous_context.get("compounds"):
            compounds = previous_context["compounds"]
            compound_list = list(compounds.keys())[:50]  # ìµœëŒ€ 50ê°œ
            if compound_list:
                system_content += f"""

**Previously discovered compounds from other sources (for reference):**
{', '.join(compound_list)}
- If these compounds appear in the images, match them with existing information
- Also extract any NEW compounds found in the images
- Maintain consistency in compound naming across all images"""
        
        messages.append({"role": "system", "content": system_content})
        
        # 3. ê° ì´ë¯¸ì§€ ë¶„ì„ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€)
        all_records = []
        logger.info(f"ì´ {len(images)}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘...")
        if previous_context:
            prev_compounds = len(previous_context.get("compounds", {}))
            logger.info(f"  ì´ì „ ë‹¨ê³„ì—ì„œ ë°œê²¬ëœ í™”í•©ë¬¼: {prev_compounds}ê°œ (ë§¥ë½ ì°¸ì¡°)")
        if use_conversation_history:
            logger.info(f"  ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€ ëª¨ë“œ: ê° ì´ë¯¸ì§€ ë¶„ì„ì´ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤")
        
        for i, image_data in enumerate(images, 1):
            logger.info(f"[{i}/{len(images)}] {image_data['class']} ë¶„ì„ ì¤‘: {image_data['filename']}...")
            try:
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if use_conversation_history:
                    records = self._analyze_image_with_history(image_data, messages, i, len(images))
                else:
                    # ê¸°ì¡´ ë°©ì‹ (ë…ë¦½ì  í˜¸ì¶œ)
                    records = self.analyze_image(image_data, previous_context=previous_context)
                
                all_records.extend(records)
                if records:
                    logger.info(f"  âœ… {len(records)}ê°œ ë ˆì½”ë“œ ì¶”ì¶œ")
                else:
                    logger.info(f"  â­ï¸  í™”í•©ë¬¼ ì •ë³´ ì—†ìŒ (ê±´ë„ˆëœ€)")
            except Exception as e:
                logger.error(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue
            
            # ì§„í–‰ë¥  í‘œì‹œ (10ê°œë§ˆë‹¤)
            if i % 10 == 0:
                logger.info(f"ì§„í–‰ë¥ : {i}/{len(images)} ({i*100//len(images)}%), í˜„ì¬ê¹Œì§€ {len(all_records)}ê°œ ë ˆì½”ë“œ ì¶”ì¶œë¨")
        
        # 3. í™”í•©ë¬¼ë³„ë¡œ ê·¸ë£¹í™” ë° ì¤‘ë³µì œê±°
        compounds_dict = {}
        seen = set()
        
        for record in all_records:
            comp_name = record.get("compound_name", "").strip()
            indicator = record.get("indicator_name", "").strip()
            value = record.get("value", "").strip()
            
            if not comp_name or not indicator or not value:
                continue
            
            dup_key = (comp_name.lower(), indicator.lower(), value.lower())
            if dup_key in seen:
                continue
            seen.add(dup_key)
            
            if comp_name not in compounds_dict:
                compounds_dict[comp_name] = {
                    "compound_name": comp_name,
                    "attributes": {},
                    "aliases": []
                }
            
            if indicator in compounds_dict[comp_name]["attributes"]:
                existing = compounds_dict[comp_name]["attributes"][indicator]
                if isinstance(existing.get("value"), list):
                    if value not in existing["value"]:
                        existing["value"].append(value)
                        existing["source"] = f"{existing['source']}, {record.get('source', '')}"
                else:
                    if existing.get("value") != value:
                        compounds_dict[comp_name]["attributes"][indicator] = {
                            "value": [existing["value"], value],
                            "unit": existing.get("unit", "") or record.get("unit", ""),
                            "source": f"{existing['source']}, {record.get('source', '')}"
                        }
                    else:
                        existing["source"] = f"{existing['source']}, {record.get('source', '')}"
            else:
                compounds_dict[comp_name]["attributes"][indicator] = {
                    "value": value,
                    "unit": record.get("unit", ""),
                    "source": record.get("source", "")
                }
        
        compounds = list(compounds_dict.values())
        total_attrs = sum(len(c.get("attributes", {})) for c in compounds)
        
        result = {
            "compounds": compounds,
            "summary": {
                "total_compounds": len(compounds),
                "total_attributes": total_attrs,
                "total_records": len(all_records),
                "images_processed": len(images)
            },
            "raw_jsonl": all_records
        }
        
        logger.info(f"ì¶”ì¶œ ì™„ë£Œ: í™”í•©ë¬¼ {len(compounds)}ê°œ, ì´ ì†ì„± {total_attrs}ê°œ, ë ˆì½”ë“œ {len(all_records)}ê°œ, ì´ë¯¸ì§€ {len(images)}ê°œ")
        
        return result
    
    def process_pdf_graph(self, pdf_graph_dir: Path, output_dir: Path, 
                        previous_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """PDF ê·¸ë˜í”„ ë¶„ì„ ë° ê²°ê³¼ ì €ì¥
        
        Args:
            pdf_graph_dir: PDF ê·¸ë˜í”„ ë””ë ‰í† ë¦¬
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            previous_context: ì´ì „ ë‹¨ê³„ì—ì„œ ëˆ„ì ëœ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¶„ì„ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
        result = self.analyze_pdf_graph(pdf_graph_dir, previous_context=previous_context)
        
        # ê²°ê³¼ ì €ì¥
        pdf_name = pdf_graph_dir.name
        output_file = output_dir / f"{pdf_name}_yolo_gpt_analysis.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
        
        return result


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="YOLOë¡œ ì¶”ì¶œí•œ ì´ë¯¸ì§€ë¥¼ GPT-4o Visionìœ¼ë¡œ ë¶„ì„")
    parser.add_argument("--pdf_graph_dir", required=True, help="pdf_graph í´ë” ê²½ë¡œ (ì˜ˆ: supp_extracted/PMC7066191/pdf_graph/41467_2020_15111_MOESM1_ESM)")
    parser.add_argument("--output_dir", help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: pdf_graph_dir ìƒìœ„ í´ë”/pdf_info)")
    parser.add_argument("--pmc_id", help="PMC ID (ì¶œë ¥ ê²½ë¡œ ì§€ì •ìš©)")
    
    args = parser.parse_args()
    
    pdf_graph_dir = Path(args.pdf_graph_dir)
    if not pdf_graph_dir.exists():
        print(f"âŒ PDF ê·¸ë˜í”„ í´ë” ì—†ìŒ: {pdf_graph_dir}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²°ì •
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        # supp_extracted/PMC###/pdf_info/ í˜•íƒœë¡œ ì €ì¥
        if args.pmc_id:
            output_dir = pdf_graph_dir.parent.parent / "pdf_info"
        else:
            output_dir = pdf_graph_dir.parent / "pdf_info"
    
    analyzer = YOLOImageAnalyzer()
    result = analyzer.process_pdf_graph(pdf_graph_dir, output_dir)
    
    compound_count = len(result.get("compounds", []))
    images_processed = result.get("summary", {}).get("images_processed", 0)
    print(f"âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ!")
    print(f"  í´ë”: {pdf_graph_dir.name}")
    print(f"  í™”í•©ë¬¼ ìˆ˜: {compound_count}")
    print(f"  ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {images_processed}ê°œ")
    print(f"  ê²°ê³¼: {output_dir}")


if __name__ == "__main__":
    main()

