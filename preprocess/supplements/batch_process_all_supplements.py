#!/usr/bin/env python3
"""
ë³´ì¶©ìë£Œ í†µí•© ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- data_test/supp/PMC###/ ì•„ë˜ ëª¨ë“  íŒŒì¼ ìë™ ê°ì§€
- Excel/Word/PDF íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë™ì  ì²˜ë¦¬
- ê²°ê³¼ë¥¼ supp_extracted/PMC###/ ì•„ë˜ í†µí•© ì €ì¥
"""

import json
import logging
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def find_supplement_files(supp_dir: Path, pmc_id: str) -> Dict[str, List[Path]]:
    """ë³´ì¶©ìë£Œ í´ë”ì—ì„œ ëª¨ë“  íŒŒì¼ ì°¾ê¸° (íƒ€ì…ë³„ ë¶„ë¥˜)"""
    pmc_supp_dir = supp_dir / pmc_id
    if not pmc_supp_dir.exists():
        return {}
    
    files = {
        'excel': [],
        'word': [],
        'pdf': []
    }
    
    # Excel íŒŒì¼
    for ext in ['.xlsx', '.xls']:
        files['excel'].extend(list(pmc_supp_dir.glob(f"*{ext}")))
    
    # Word íŒŒì¼
    for ext in ['.docx', '.doc']:
        files['word'].extend(list(pmc_supp_dir.glob(f"*{ext}")))
    
    # PDF íŒŒì¼
    files['pdf'].extend(list(pmc_supp_dir.glob("*.pdf")))
    
    # ì •ë ¬
    for file_type in files:
        files[file_type] = sorted(files[file_type])
    
    return files


def process_excel(excel_path: Path, pmc_id: str, output_dir: Path, 
                  llama_normalize: bool = False) -> Dict[str, Any]:
    """Excel íŒŒì¼ ì²˜ë¦¬ (Llama ì‚¬ìš© ì•ˆí•¨)"""
    logger.info(f"  ğŸ“Š Excel ì²˜ë¦¬: {excel_path.name}")
    
    try:
        # Llama ì‚¬ìš© ì•ˆí•¨ (ê¸°ë³¸ê°’ False)
        result = subprocess.run(
            [
                sys.executable, "extract_excel_supplements.py",
                str(excel_path),
                "--output_dir", str(output_dir / "excel")
                # --llama-normalize ì˜µì…˜ ì œê±° (ì‚¬ìš© ì•ˆí•¨)
            ],
            capture_output=True,
            text=True,
            timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode == 0:
            logger.info(f"    âœ… Excel ì²˜ë¦¬ ì™„ë£Œ: {excel_path.name}")
            return {"status": "success", "file": excel_path.name}
        else:
            logger.error(f"    âŒ Excel ì²˜ë¦¬ ì‹¤íŒ¨: {excel_path.name}")
            logger.error(f"      stderr: {result.stderr[-500:]}")
            return {"status": "failed", "file": excel_path.name, "error": result.stderr[-500:]}
    except subprocess.TimeoutExpired:
        logger.error(f"    âŒ Excel ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ: {excel_path.name}")
        return {"status": "timeout", "file": excel_path.name}
    except Exception as e:
        logger.error(f"    âŒ Excel ì²˜ë¦¬ ì˜¤ë¥˜: {excel_path.name} - {e}")
        return {"status": "error", "file": excel_path.name, "error": str(e)}


def process_word(word_path: Path, pmc_id: str, output_dir: Path) -> Dict[str, Any]:
    """Word íŒŒì¼ ì²˜ë¦¬"""
    logger.info(f"  ğŸ“ Word ì²˜ë¦¬: {word_path.name}")
    
    try:
        result = subprocess.run(
            [
                sys.executable, "extract_word_supplements.py",
                str(word_path),
                "--output_dir", str(output_dir / "word")
            ],
            capture_output=True,
            text=True,
            timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode == 0:
            logger.info(f"    âœ… Word ì²˜ë¦¬ ì™„ë£Œ: {word_path.name}")
            return {"status": "success", "file": word_path.name}
        else:
            logger.error(f"    âŒ Word ì²˜ë¦¬ ì‹¤íŒ¨: {word_path.name}")
            logger.error(f"      stderr: {result.stderr[-500:]}")
            return {"status": "failed", "file": word_path.name, "error": result.stderr[-500:]}
    except subprocess.TimeoutExpired:
        logger.error(f"    âŒ Word ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ: {word_path.name}")
        return {"status": "timeout", "file": word_path.name}
    except Exception as e:
        logger.error(f"    âŒ Word ì²˜ë¦¬ ì˜¤ë¥˜: {word_path.name} - {e}")
        return {"status": "error", "file": word_path.name, "error": str(e)}


def process_pdf(pdf_path: Path, pmc_id: str, output_dir: Path,
                skip_completed: bool = False,
                previous_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """PDF íŒŒì¼ ì²˜ë¦¬ (YOLO ì¶”ì¶œ + GPT-4o Vision ë¶„ì„)
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        pmc_id: PMC ID
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        skip_completed: ì™„ë£Œëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°
        previous_context: ì´ì „ ë‹¨ê³„ì—ì„œ ëˆ„ì ëœ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
    """
    logger.info(f"  ğŸ“„ PDF ì²˜ë¦¬: {pdf_path.name}")
    
    pdf_name = pdf_path.stem
    
    # ì¶œë ¥ ê²½ë¡œ
    pdf_graph_dir = output_dir / "pdf_graph" / pdf_name
    pdf_info_dir = output_dir / "pdf_info"
    output_file = pdf_info_dir / f"{pdf_name}_yolo_gpt_analysis.json"
    
    # ì´ë¯¸ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if skip_completed and output_file.exists():
        logger.info(f"    â­ï¸  ì´ë¯¸ ì™„ë£Œë¨, ê±´ë„ˆëœ€")
        return {"status": "skipped", "file": pdf_path.name}
    
    # 1. YOLOë¡œ figure/table ì¶”ì¶œ
    extracted = False
    if pdf_graph_dir.exists() and (
        (pdf_graph_dir / "figures").exists() and any((pdf_graph_dir / "figures").iterdir()) or
        (pdf_graph_dir / "tables").exists() and any((pdf_graph_dir / "tables").iterdir())
    ):
        logger.info(f"    âœ… ì´ë¯¸ ì¶”ì¶œë¨")
        extracted = True
    else:
        pdf_graph_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"    ğŸ” YOLO ì¶”ì¶œ ì¤‘...")
        
        try:
            result = subprocess.run(
                [
                    sys.executable, "inference_yolo.py",
                    "--pdf", str(pdf_path),
                    "--output", str(pdf_graph_dir),
                    "--confidence", "0.25"
                ],
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                extracted = True
                logger.info(f"    âœ… YOLO ì¶”ì¶œ ì™„ë£Œ")
            else:
                logger.error(f"    âŒ YOLO ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr[-500:]}")
                return {"status": "extraction_failed", "file": pdf_path.name, "error": result.stderr[-500:]}
        except subprocess.TimeoutExpired:
            logger.error(f"    âŒ YOLO ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ")
            return {"status": "extraction_timeout", "file": pdf_path.name}
        except Exception as e:
            logger.error(f"    âŒ YOLO ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {"status": "extraction_error", "file": pdf_path.name, "error": str(e)}
    
    # 2. GPT-4o Visionìœ¼ë¡œ ë¶„ì„ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
    if not extracted:
        return {"status": "extraction_failed", "file": pdf_path.name}
    
    pdf_info_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"    ğŸ¤– GPT-4o Vision ë¶„ì„ ì¤‘...")
    if previous_context:
        prev_compounds = len(previous_context.get("compounds", {}))
        logger.info(f"    ğŸ“‹ ì´ì „ ë‹¨ê³„ í™”í•©ë¬¼ {prev_compounds}ê°œ ì°¸ì¡°")
    
    try:
        from analyze_yolo_extracted_images import YOLOImageAnalyzer
        analyzer = YOLOImageAnalyzer()
        result = analyzer.process_pdf_graph(pdf_graph_dir, pdf_info_dir, previous_context=previous_context)
        
        compounds_count = len(result.get('compounds', []))
        images_count = result.get('summary', {}).get('images_processed', 0)
        
        logger.info(f"    âœ… ë¶„ì„ ì™„ë£Œ: í™”í•©ë¬¼ {compounds_count}ê°œ, ì´ë¯¸ì§€ {images_count}ê°œ")
        return {
            "status": "success",
            "file": pdf_path.name,
            "compounds": compounds_count,
            "images": images_count
        }
    except Exception as e:
        logger.error(f"    âŒ GPT-4o Vision ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {"status": "analysis_failed", "file": pdf_path.name, "error": str(e)}


def process_pmc(pmc_id: str, base_dir: Path = Path("data_test"),
                skip_completed: bool = False,
                llama_normalize_excel: bool = False) -> Dict[str, Any]:
    """íŠ¹ì • PMCì˜ ëª¨ë“  ë³´ì¶©ìë£Œ ì²˜ë¦¬ (ë§¥ë½ ëˆ„ì )"""
    logger.info(f"PMC {pmc_id} ì²˜ë¦¬ ì‹œì‘...")
    
    # ì»¨í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    from contextual_extraction_pipeline import ContextualExtractionPipeline
    pipeline = ContextualExtractionPipeline(base_dir=str(base_dir))
    context = pipeline.get_accumulated_context(pmc_id)
    
    supp_dir = base_dir / "supp"
    output_dir = base_dir / "supp_extracted" / pmc_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ì°¾ê¸°
    files = find_supplement_files(supp_dir, pmc_id)
    
    total_files = sum(len(files[ft]) for ft in files)
    if total_files == 0:
        logger.warning(f"  âš ï¸  ë³´ì¶©ìë£Œ íŒŒì¼ ì—†ìŒ")
        return {
            "pmc_id": pmc_id,
            "status": "no_files",
            "total": 0
        }
    
    logger.info(f"  ë°œê²¬ëœ íŒŒì¼:")
    logger.info(f"    Excel: {len(files['excel'])}ê°œ")
    logger.info(f"    Word: {len(files['word'])}ê°œ")
    logger.info(f"    PDF: {len(files['pdf'])}ê°œ")
    logger.info(f"    ì´ {total_files}ê°œ")
    
    results = {
        "excel": [],
        "word": [],
        "pdf": []
    }
    
    # Excel ì²˜ë¦¬ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
    for excel_path in files['excel']:
        result = process_excel(excel_path, pmc_id, output_dir, llama_normalize_excel)
        results['excel'].append(result)
        # TODO: Excel ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ëˆ„ì 
    
    # Word ì²˜ë¦¬ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
    for word_path in files['word']:
        result = process_word(word_path, pmc_id, output_dir)
        results['word'].append(result)
        # TODO: Word ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ëˆ„ì 
    
    # PDF ì²˜ë¦¬ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬ ë° ëˆ„ì )
    for pdf_path in files['pdf']:
        result = process_pdf(pdf_path, pmc_id, output_dir, skip_completed, previous_context=context)
        results['pdf'].append(result)
        
        # PDF ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ëˆ„ì 
        if result.get('status') == 'success':
            # PDF ë¶„ì„ ê²°ê³¼ ë¡œë“œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            pdf_name = pdf_path.stem
            pdf_info_file = output_dir / "pdf_info" / f"{pdf_name}_yolo_gpt_analysis.json"
            if pdf_info_file.exists():
                try:
                    with open(pdf_info_file, 'r', encoding='utf-8') as f:
                        pdf_result = json.load(f)
                    compounds = pdf_result.get('compounds', [])
                    # ì»¨í…ìŠ¤íŠ¸ì— ëˆ„ì 
                    for comp in compounds:
                        comp_name = comp.get('compound_name', '').strip()
                        if comp_name:
                            if comp_name not in context['compounds']:
                                context['compounds'][comp_name] = {
                                    'aliases': set(),
                                    'attributes': defaultdict(list),
                                    'sources': []
                                }
                            # ì†ì„± ì¶”ê°€
                            for attr_name, attr_data in comp.get('attributes', {}).items():
                                if isinstance(attr_data, dict):
                                    value = attr_data.get('value', '')
                                    if value:
                                        context['compounds'][comp_name]['attributes'][attr_name].append({
                                            'value': value,
                                            'source': f'pdf_{pdf_name}'
                                        })
                            if f'pdf_{pdf_name}' not in context['compounds'][comp_name]['sources']:
                                context['compounds'][comp_name]['sources'].append(f'pdf_{pdf_name}')
                except Exception as e:
                    logger.warning(f"  ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ì‹¤íŒ¨: {e}")
        
        # ì»¨í…ìŠ¤íŠ¸ ì €ì¥
        pipeline.save_accumulated_context(pmc_id, context)
    
    # í†µê³„
    excel_success = sum(1 for r in results['excel'] if r.get('status') == 'success')
    word_success = sum(1 for r in results['word'] if r.get('status') == 'success')
    pdf_success = sum(1 for r in results['pdf'] if r.get('status') == 'success')
    
    total_success = excel_success + word_success + pdf_success
    
    logger.info(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ:")
    logger.info(f"    Excel: {excel_success}/{len(files['excel'])}")
    logger.info(f"    Word: {word_success}/{len(files['word'])}")
    logger.info(f"    PDF: {pdf_success}/{len(files['pdf'])}")
    logger.info(f"    ì´: {total_success}/{total_files}")
    
    return {
        "pmc_id": pmc_id,
        "status": "completed",
        "total_files": total_files,
        "success": total_success,
        "results": results
    }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ë³´ì¶©ìë£Œ í†µí•© ì²˜ë¦¬ (Excel/Word/PDF ìë™ ê°ì§€)")
    parser.add_argument("--pmc_id", help="íŠ¹ì • PMC IDë§Œ ì²˜ë¦¬")
    parser.add_argument("--base_dir", default="data_test", help="ê¸°ë³¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--skip-completed", action="store_true", help="ì´ë¯¸ ì™„ë£Œëœ PDF ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--llama-normalize", action="store_true", help="Excel ì†ì„± ì •ê·œí™”ì— Llama ì‚¬ìš©")
    parser.add_argument("--limit", type=int, help="ì²˜ë¦¬í•  ìµœëŒ€ PMC ê°œìˆ˜")
    parser.add_argument("--start", type=int, default=0, help="ì‹œì‘ ì¸ë±ìŠ¤")
    
    args = parser.parse_args()
    
    base_dir = Path(args.base_dir)
    supp_dir = base_dir / "supp"
    
    if not supp_dir.exists():
        logger.error(f"ë³´ì¶©ìë£Œ í´ë” ì—†ìŒ: {supp_dir}")
        sys.exit(1)
    
    # PMC í´ë” ì°¾ê¸°
    if args.pmc_id:
        pmc_ids = [args.pmc_id]
    else:
        pmc_ids = sorted([d.name for d in supp_dir.iterdir() 
                         if d.is_dir() and d.name.startswith('PMC')])
    
    total = len(pmc_ids)
    logger.info(f"ì´ {total}ê°œ PMC í´ë” ë°œê²¬")
    
    # ì‹œì‘ ì¸ë±ìŠ¤ë¶€í„°
    pmc_ids = pmc_ids[args.start:]
    if args.limit:
        pmc_ids = pmc_ids[:args.limit]
    
    success_count = 0
    total_files = 0
    total_success = 0
    
    for i, pmc_id in enumerate(pmc_ids, args.start + 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"[{i}/{total}] PMC {pmc_id} ì²˜ë¦¬ ì¤‘...")
        logger.info(f"{'='*80}")
        
        try:
            result = process_pmc(pmc_id, base_dir, args.skip_completed, args.llama_normalize)
            
            if result.get("status") == "completed":
                success_count += 1
                total_files += result.get("total_files", 0)
                total_success += result.get("success", 0)
            else:
                logger.warning(f"  âš ï¸  ìƒíƒœ: {result.get('status')}")
        except Exception as e:
            logger.error(f"  âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    logger.info(f"  ì„±ê³µí•œ PMC: {success_count}ê°œ")
    logger.info(f"  ì²˜ë¦¬ëœ íŒŒì¼: {total_success}/{total_files}ê°œ")
    logger.info(f"{'='*80}")


if __name__ == "__main__":
    main()

